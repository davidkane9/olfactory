---
title: "Olfactory"
author: "David Kane"
date: "1/26/2019"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(tidyverse)
library(cowplot)
library(stargazer)
load("AJPSReplication.RData")

# It is good practice to save a permanent copy of the original data so that you
# don't need to reload it each time. Does not matter in this case, because the
# data is so small.

orig <- x
x <- as_tibble(orig) %>%
  
  # Not sure why these IDs come in as ordered factors, but characters would be
  # better. My names are much less confusing.
  
  mutate(e_id = as.character(ideval_n),
         t_id = as.character(type_n)) %>% 
  
  # Rename some variables
  
  rename(t_gender = MaleTarget,
         e_gender = Male,
         t_ideo_scale = IdeoTarget,
         e_ideo_scale = politicalIdeo) %>% 
  
  # Need the 0/1 version of the ideology variable for both targets and
  # evaluators.
  
  mutate(t_ideology = ifelse(t_ideo_scale >= 4, "Conservative", "Liberal")) %>% 
  mutate(e_ideology = ifelse(e_ideo_scale >= 4, "Conservative", "Liberal")) %>% 
  
  # Create variables used in the regression. 
  
  mutate(same_ideology = (t_ideology == e_ideology)) %>% 
  
  # I am concerned that we are given a variable called MaleMaleTarget when the
  # regression seems to be using a variable labeled "Same Sex."
  
  mutate(same_sex = (t_gender == e_gender)) %>% 
  
  # I find this negative absolute measure confusing.
  
  mutate(neg_abs_ideo_diff = - abs(t_ideo_scale - e_ideo_scale)) %>% 
  
  # Rearrange for neatness.
  
  select(attractive, e_id, t_id, 
         e_gender, t_gender, 
         e_ideo_scale, t_ideo_scale,
         e_ideology, t_ideology,
         same_sex, same_ideology,
         neg_abs_ideo_diff)
  
  
```


```{r}
# Models 1 and 2 use average target/evaluator attractiveness. I find this
# confusing. First, only one of them is included in the provided data. Second,
# although including fixed effects might be sensible, as in Model 3, I don't see
# a reason why these means are useful predictors. Anyway, I calculate them here.
# My t_attractiveness matches the provided mn_attractive well.

t_x <- x %>% 
  group_by(t_id) %>% 
  summarize(t_attractiveness = mean(attractive, na.rm = TRUE))

e_x <- x %>% 
  group_by(e_id) %>% 
  summarize(e_attractiveness = mean(attractive, na.rm = TRUE))

# Isn't there an easy of adding summary data like this to a tibble? This
# completes the data set up.

x <- left_join(x, t_x, by = "t_id") %>% 
  left_join( e_x, by = "e_id")

```


# Data Introduction

There are two id variables: `ideval_n` (which I rename to `e_id`) identifies the evaluators and `type_n` (which I rename as `t_id`) identifies the targets. In the R data frame which I grabbed from the Dataverse, they come in as ordered factors. That is clearly wrong, but I doubt that this was an issue in the published paper.

The data set includes `r nrow(x)` rows. There were `r length(unique(x$t_id))` "target" individuals whose smell was evaluated by `r length(unique(x$e_id))` evaluators. The `r nrow(x)` rows is a result of `r length(unique(x$t_id))` times `r length(unique(x$e_id))`.  However, it *looks* like the target individuals *also* served as evaluators. I don't see this mentioned in the paper, but what else would explain why some of the values for `e_id` are also present in `t_id`? Without *any* codebook with clear variable explanations, it is tough to know. 

The `r sum(is.na(x$attractive))` missing values for the response variable (a measure of attractiveness) are a bit of a mystery. The paper discusses dropping one target because he wore the pads for two days instead of one, but my sense is that none of his data should be here. The missing values for `attractive` are spread not-exactly-evenly across all 21 targets. This number of missing is consistent with the regression results, which only show 2,195 observations, but I am still curious about the reason for the missing values. A proper forensic examination would look more closely at this.


## Targets

We can replicate the SI-1 table about targets perfectly. Ought to explore the `gt` package in order to make these look nicer.

```{r}
# The data for each target is repeated 119 times, one row for each evaluator.
# So, to get information for just the targets, we just subset out each 119 rows.
# I think that sex and ideology are the only things we know about targets.

targets <- x %>% 
  slice(seq(1, 2499, by = 119)) %>% 
  select(t_id, t_gender, t_ideo_scale, t_ideology)

z <- addmargins(table(targets$t_gender, targets$t_ideology))
```


## Evaluators

We are one off when trying to replicate the data for evaluators. We have one extra male Conservative.

```{r}
# The data for the evaluators is repeated for each new target. So, we can just
# look at the first 119 rows of the original data frame.

evaluators <- x %>% 
  slice(1:119) %>% 
  select(e_id, e_gender, e_ideo_scale, e_ideology)
  

addmargins(table(evaluators$e_gender, evaluators$e_ideology))  
```

My *guess* is that this is a coding mistake in which the original paper drops an evaluator with a `politicalIdeo` value of 4, instead of classifying him as a Conservative. I doubt it matters.

UPDATE: I contacted Dustin Tingley. He confirms that the published result is mistaken and that my numbers are correct.

# Figure 1

Here are some rough versions of the component parts of Figure 1.

```{r fig1, fig.cap = "*Note*: The sexual categories are binary, and the ideological ones exist on a 7-point scale ranging from *very liberal* (1) to *very conservative* (7)."}

p1 <- ggplot(targets, aes(x = t_ideo_scale)) + 
  geom_bar() +
  ggtitle("Target Ideology") +
  xlab("Liberal : Conservative") +
  ylab("Frequency")

p2 <- ggplot(targets, aes(x = t_gender)) + 
  geom_bar() +
  ggtitle("Target Sex") +
  ylab("Frequency") +
  theme(axis.title.x = element_blank())

p3 <- ggplot(evaluators, aes(x = e_ideo_scale)) + 
  geom_bar() +
  ggtitle("Evaluator Ideology") +
  xlab("Liberal : Conservative") +
  ylab("Frequency")

p4 <- ggplot(evaluators, aes(x = e_gender)) + 
  geom_bar() +
  ggtitle("Evaluator Sex") +
  ylab("Frequency") +
  theme(axis.title.x = element_blank())

cowplot::plot_grid(p1, p3, p2, p4, nrow = 2)
```

Looking by eyeball, these seem to match the published plots. Note the single evaluator with a `politicalIdeo` value of 4. I think that this individual is mistakenly dropped from Table SI-1 but is present in these figures. The `cowplot` package would be helpful in grouping these plots together. We might also mess around with the x-axis tick labels to see all 7 values. Not sure what the point is of making the gender bars more narrow. 


# Table 1

For now, I don't bother with clustered standard errors.

```{r regressions, echo=TRUE}
mod_1 <- lm(data = x, attractive ~ same_ideology + same_sex + 
             e_ideology + t_ideology +
             e_gender   + t_gender +
             e_attractiveness + t_attractiveness)


mod_2 <- lm(data = x, attractive ~ neg_abs_ideo_diff + same_sex + 
              e_ideo_scale + t_ideo_scale + 
              e_gender + t_gender + 
              e_attractiveness + t_attractiveness)

mod_3 <- lm(data = x, attractive ~ neg_abs_ideo_diff + same_sex + t_id + e_id)
```

```{r stargazer, results='asis', fig.cap="*Note*: Standard errors are not clustered, unlike the published version."}
stargazer(mod_1, mod_2, mod_3,
          header = FALSE,
          title = "Odor Attraction as a Function of Ideological Similarity",
          style = "ajps",
          keep = c("same_ideology", "neg_abs_ideo_diff", "same_sex",
                   "e_ideology", "t_ideology", 
                   "e_ideo_scale", "t_ideo_scale",
                   "e_gender", "t_gender",
                   "t_attractiveness", "e_attractiveness"),
          covariate.labels = c("Same Ideology", "-Abs. Ideology Diff.", "Same Sex",
                               "Conservative Eval.", "Conservative Target", "Ideology of Eval.",
                               "Ideology of Target", "Male Evaluator", "Male Target",
                               "Avg. Target Attract", "Avg Eval. Attract"))
```


Note how closely the key result, the coefficient and standard error for `neg_abs_ideo_diff` matches the published version in Model 3.
